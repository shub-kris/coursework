{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Mean-field VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Translating the math in slide 25 of lecture 22 to an efficient piece of code.\n",
    "\n",
    "**Dataset:** State of the Union---same as the previous exercise.\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "1. `pip install tqdm, sklearn, scipy, numpy, spacy, matplotlib`\n",
    "2. `python -m spacy download en_core_web_sm`\n",
    "3. Unzip `Exercise_10_data.zip`, make sure you have the following directory structure: `./data/sotu/`\n",
    "\n",
    "**Useful external references:**\n",
    "1. https://arxiv.org/abs/1601.00670 (Also very useful for supplementing what you have seen in the lecture.)\n",
    "2. http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pre-processed texts from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 500)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "from scipy.special import digamma\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "SOTU_DIR = DATA_DIR + 'sotu/'\n",
    "\n",
    "# Reduce these values to make things faster, e.g. for debugging\n",
    "MAX_DOC_LEN = 1000\n",
    "VOCAB_SIZE = 500\n",
    "\n",
    "\n",
    "with open(DATA_DIR+'years.bin', 'rb') as f:\n",
    "    years = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR+'preprocessed.bin', 'rb') as f:\n",
    "    preprocessed = pickle.load(f)\n",
    "\n",
    "count_vect = CountVectorizer(max_features=VOCAB_SIZE, stop_words=['000'])\n",
    "X = count_vect.fit_transform(preprocessed)\n",
    "\n",
    "X = np.asarray(X.todense())\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "word2idx = count_vect.vocabulary_\n",
    "idx2word = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the features matrix that we are going to use during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(preprocessed)  # num of docs\n",
    "V = count_vect.max_features  # vocabulary size\n",
    "\n",
    "W = np.zeros([D, MAX_DOC_LEN, V], dtype='int8')  # Dataset\n",
    "\n",
    "for d, doc in enumerate(preprocessed):\n",
    "    w = []\n",
    "    \n",
    "    for word in doc.split(' '):\n",
    "        try:\n",
    "            idx = word2idx[word]\n",
    "            w.append(idx)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        if len(w) >= MAX_DOC_LEN:\n",
    "            break\n",
    "            \n",
    "    for i, w_ in enumerate(w):\n",
    "        W[d, i, w_] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA inference with VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Gibbs sampling (previous exercise), we _sample_ latent variables from the LDA posterior (the LDA model conditioned to documents). In VI, we do a _full distribution approximation_ to the posterior instead. Specifically, we put a simplifying (mean-field) assumption on our model and optimize the reverse KL-divergence between the model and the true posterior.\n",
    "\n",
    "VI can be seen as casting inference into optimization. While one can use any optimization algorithm to maximize the variational bound (ELBO), here we will implement the simplest algorithm for doing so. The algorithm is termed coordinate-ascent variational-inference (CAVI). It updates each variational distribution in turn using the full-conditional of the corresponding variable. The resulting algorithm is very similar to Gibbs sampling, but to emphasize it again: we do a _full distribution approximation_ to the posterior, and not _sampling_.\n",
    "\n",
    "**Hint.** The pseudocode for the aforementioned algorithm is in slide 25 of lecture 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with some hyperparameters, similar to the previous exercise sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10  # num of topics\n",
    "\n",
    "# Dirichlet priors\n",
    "alpha = 1/K\n",
    "beta = 1/K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1.** Initialize the variational parameters of the three variational distributions:\n",
    "\n",
    "* $\\mathbf{\\tilde{\\alpha}}_d$, the variational parameter of $q(\\mathbf{\\pi}_d)$\n",
    "* $\\mathbf{\\tilde{\\beta}}_k$, the variational parameter of $q(\\mathbf{\\theta}_k)$\n",
    "* $\\mathbf{\\tilde{\\gamma}}_{di}$, the variational parameter of $q(\\mathbf{c}_{di})$\n",
    "\n",
    "Vectorize as much as you can---the data structure you choose affects the optimization runtime.\n",
    "\n",
    "Note again: These variables are parameters to the variational distributions, not for holding samples like in Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # gamma_tilde := variational parameters of the distributions of the document-word topic assigments\n",
    "    gamma_tilde = np.zeros((D, MAX_DOC_LEN, K))\n",
    "    \n",
    "    for d in range(len(W_idx)):\n",
    "        w = W_idx[d]\n",
    "        for i in range(len(w)):\n",
    "            gamma_tilde[d, i, :] = np.random.dirichlet(np.ones(K) * alpha)\n",
    "        \n",
    "    # alpha_tilde := variational parameters of the document-topic distributions\n",
    "    alpha_tilde = np.ones((D, K)) * alpha\n",
    "\n",
    "    # beta_tilde := variational parameters of the topic-word distributions\n",
    "    beta_tilde = np.ones((K, V)) * beta\n",
    "    \n",
    "    return alpha_tilde, beta_tilde, gamma_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will \"translate\" the pseudocode in the lecture slides into a real (efficient!) code. \n",
    "\n",
    "**TASK 2.** Inside the CAVI loop, update $(\\mathbf{\\tilde{\\alpha}}_d), (\\mathbf{\\tilde{\\beta}}_k), (\\mathbf{\\tilde{\\gamma}}_{di})$ in turn. Vectorize the update as much as you can!\n",
    "\n",
    "Note that, the computation of the ELBO is not necessary and you can skip this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(N_ITER=20):\n",
    "    \n",
    "    alpha_tilde, beta_tilde, gamma_tilde = init_params()\n",
    "\n",
    "    for it in tqdm.trange(N_ITER):\n",
    "\n",
    "        # Update alpha_tilde\n",
    "        # ------------------\n",
    "        alpha_tilde = alpha + np.sum(gamma_tilde, axis=1)\n",
    "\n",
    "        # Update beta_tilde\n",
    "        # -----------------\n",
    "        beta_tilde = beta + np.tensordot(gamma_tilde, W, axes=([0, 1], [0, 1]))\n",
    "\n",
    "        # Update gamma_tilde\n",
    "        # ------------------\n",
    "        \n",
    "        # Pre-compute digamma functions\n",
    "        dg_alpha = digamma(alpha_tilde)\n",
    "        dg_beta_1 = digamma(beta_tilde)\n",
    "        dg_beta_2 = digamma(beta_tilde.sum(axis=1))\n",
    "        \n",
    "        for d in range(D):\n",
    "            \n",
    "            # Get list of word indices\n",
    "            w = W_idx[d]\n",
    "\n",
    "            for i in range(len(w)):\n",
    "                \n",
    "                # Get vocabulary word index\n",
    "                v = w[i]\n",
    "\n",
    "                for k in range(K):\n",
    "                    \n",
    "                    # Compute digamma sum\n",
    "                    gamma_tilde[d, i, k] = dg_alpha[d, k] + dg_beta_1[k, v] - dg_beta_2[k]\n",
    "\n",
    "                # Normalize\n",
    "                gamma_tilde[d, i, :] = softmax(gamma_tilde[d, i, :])\n",
    "\n",
    "    return alpha_tilde, beta_tilde, gamma_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-25382b3c89c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-32d021c49f91>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(N_ITER)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_ITER\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0malpha_tilde\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_tilde\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_tilde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_ITER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a6e068c8f132>\u001b[0m in \u001b[0;36minit_params\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgamma_tilde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_DOC_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W_idx' is not defined"
     ]
    }
   ],
   "source": [
    "a, b, g = run(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have successfully found all the variational parameters (and thus the variational distributions), we are ready to evaluate the inferred latent variables.\n",
    "\n",
    "**TASK 3.** Print the _expected_ top-$n$ words for each topic, weigthed by its importance! To do this, recall, for each topic $k$, which variational distribution approximates the $k$-th topic-word distribution. See the previous exercise sheet (Sheet 10) for the explanation of \"importance weigthing\".\n",
    "\n",
    "**Hint.** Expected result:\n",
    "\n",
    "```\n",
    "Topic-1\n",
    "company dollar financial century income wage cost export\n",
    "\n",
    "Topic-2\n",
    "commissioner states vessel naval convention minister island treaty\n",
    "\n",
    "Topic-3\n",
    "china cent currency bond expenditure surplus bank gold\n",
    "\n",
    "Topic-4\n",
    "enemy fight war defense production military strength army\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "\n",
    "##########################################################\n",
    "##                                                      ##\n",
    "##                  YOUR CODE HERE                      ##\n",
    "##                                                      ##\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as before, let us visualize the change of topics over time.\n",
    "\n",
    "**TASK 4.** Plot the _expected_ document-topic proportion sequentially, ordered by year. See Fig. 5 in https://arxiv.org/abs/1110.4713 for an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##                                                      ##\n",
    "##                  YOUR CODE HERE                      ##\n",
    "##                                                      ##\n",
    "##########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
