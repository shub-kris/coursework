{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Translating the math to an efficient piece of code.\n",
    "\n",
    "**Dataset:** State of the Union, an annual address by the President of the United States before a joint session of congress. This dataset contains the full text of the State of the Union address from 1791 to 2018.\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "1. `pip install tqdm, sklearn, scipy, numpy, spacy, matplotlib`\n",
    "2. `python -m spacy download en_core_web_sm`\n",
    "3. Unzip `Exercise_10_data.zip`, make sure you have the following directory structure: `./data/sotu/`\n",
    "\n",
    "**Useful external references:**\n",
    "1. Murphy, \"Machine Learning: A Probabilistic Perspective\", Ch. 27.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import tqdm.auto as tqdm\n",
    "import pickle\n",
    "from scipy.special import digamma, logsumexp\n",
    "\n",
    "\n",
    "DATA_DIR = './data/'\n",
    "SOTU_DIR = DATA_DIR + 'sotu/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are doing standard preprocessing steps for text data, namely \n",
    "* stopword and punctuation removal\n",
    "* word lemmatization/stemming\n",
    "* tokenization\n",
    "* converting text into features\n",
    "\n",
    "While these steps are not our focus in this exercise, it is very rewarding to study them, since they are standard stuffs in real world applications.\n",
    "\n",
    "We are using the wonderful [spaCy](https://spacy.io/) library to do this. Once the data are preprocessed, we will store them in files for future uses, since these steps only need to be done once. So:\n",
    "\n",
    "**Skip the following cell if you have already successfully run it, to save time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9372779423fc480e90021be8130beeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=228.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "years = []  # Useful for visualization\n",
    "preprocessed = []\n",
    "\n",
    "for fname in tqdm.tqdm(os.listdir(SOTU_DIR)):\n",
    "    if not fname.endswith('.txt'):\n",
    "        continue\n",
    "        \n",
    "    years.append(int(fname[-8:-4]))  # Extract the year from the filename\n",
    "        \n",
    "    with open(SOTU_DIR+fname, 'r') as f:\n",
    "        doc = nlp(f.read())\n",
    "        bow = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct:\n",
    "                continue\n",
    "                \n",
    "            bow.append(token.lemma_)\n",
    "\n",
    "        preprocessed.append(' '.join(bow))\n",
    "        \n",
    "with open(DATA_DIR+'years.bin', 'wb') as f:\n",
    "    pickle.dump(years, f)\n",
    "        \n",
    "with open(DATA_DIR+'preprocessed.bin', 'wb') as f:\n",
    "    pickle.dump(preprocessed, f)\n",
    "    \n",
    "print(f'Saved as: {DATA_DIR+\"years.bin\"} and {DATA_DIR+\"preprocessed.bin\"}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the saved, preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR+'years.bin', 'rb') as f:\n",
    "    years = pickle.load(f)\n",
    "\n",
    "with open(DATA_DIR+'preprocessed.bin', 'rb') as f:\n",
    "    preprocessed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our goal is to build a vocabulary, assign an ID to each word in the vocabulary, and convert the documents (sequence of words) into features (sequence of word ID). Again, this is not our focus, so we will just use scikit-learn for doing this, but it is definitely worth your effort to study this step.\n",
    "\n",
    "**Hint.** You might want to tune `MAX_DOC_LEN` and `VOCAB_SIZE` to make things faster. For the final submission, please be reasonable with this values---do not set them to values that are too small, e.g. < 100!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Reduce these values to make things faster, e.g. for debugging\n",
    "MAX_DOC_LEN = 1000\n",
    "VOCAB_SIZE = 500\n",
    "\n",
    "count_vect = CountVectorizer(max_features=VOCAB_SIZE, stop_words=['000'])\n",
    "count_vect = count_vect.fit(preprocessed)\n",
    "\n",
    "word2idx = count_vect.vocabulary_\n",
    "idx2word = count_vect.get_feature_names()\n",
    "\n",
    "W = []\n",
    "\n",
    "for doc in preprocessed:\n",
    "    w = []\n",
    "    \n",
    "    for word in doc.split(' '):\n",
    "        if len(w) >= MAX_DOC_LEN:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            idx = word2idx[word]\n",
    "            w.append(idx)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    W.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA inference via Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With preprocessing stuffs done, we are ready to actually implement a Gibbs sampling algorithm for doing LDA inference. But first, here are some hyperparameters. Note that the standard choice of the Dirichlet priors' parameters $\\alpha, \\beta$ is $1/K$, where $K$ is the number of topics. This is just a heuristic; if you are curious what does this hyperparameters actually means, you can follow this wonderful lecture by David Blei from Machine Learning Summer School 2009 here: http://videolectures.net/mlss09uk_blei_tm/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VOCAB_SIZE  # vocabulary size\n",
    "D = len(W)  # num of docs\n",
    "K = 10  # num of topics\n",
    "\n",
    "# Dirichlet priors\n",
    "alpha = 1/K\n",
    "beta = 1/K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1.** Here's your first task: Initialize the necessary quantities, namely \n",
    "\n",
    "* $\\mathbf{\\Pi}$, the document-topic proportions\n",
    "* $\\mathbf{\\Theta}$ the topic-word proportions\n",
    "* $\\mathbf{C}$ the document-word topic assignments\n",
    "\n",
    "by sampling from the priors and following the generative process.\n",
    "\n",
    "**Hint.** You are not required to follow the definitions in the slides. In fact, how you define those variables might affect the algorithm's efficiency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi := document topic distribution (D, K)\n",
    "Pi = np.ones((D, K)) * alpha\n",
    "\n",
    "# Theta := topic-word distribution (K, V)\n",
    "Theta = np.ones((K, V)) * beta\n",
    "\n",
    "ks = np.arange(K)\n",
    "\n",
    "# C := document's words topic assignments (D, Id, K)\n",
    "C = list()\n",
    "\n",
    "# Initialize n\n",
    "n = np.zeros((D, K, V))\n",
    "\n",
    "for d in range(D):\n",
    "    \n",
    "    # Get next document (list of words)\n",
    "    w = W[d]\n",
    "    \n",
    "    # Get document length\n",
    "    Id = len(w)\n",
    "    \n",
    "    # Initialize c for document d\n",
    "    c = np.zeros((Id, K))\n",
    "    \n",
    "    # Get Dirichlet distribution for pi\n",
    "    pi = Pi[d, :]\n",
    "    \n",
    "    for i in range(Id):\n",
    "        \n",
    "        # Get word index in the vocabulary\n",
    "        v = w[i]\n",
    "        \n",
    "        # Get k from a categorical distribution\n",
    "        k = np.random.choice(ks, p=pi)\n",
    "        \n",
    "        # Assign topic to word in a document\n",
    "        c[i, k] = 1\n",
    "        \n",
    "        # Update (document, topic, word) counter\n",
    "        n[d, k, v] += 1\n",
    "        \n",
    "    # Append list\n",
    "    C.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2.** Now we need to _efficiently_ implement the Gibbs sampler. The code skeleton is provided---you only need to \"translate\" the mathematical expressions in the full-conditionals of $\\mathbf{\\Pi}, \\mathbf{\\Theta}, \\mathbf{C}$ into a code. You should use the lecture slides as a reference.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. Implement the Gibbs sampler naively first. It will be really slow but it is a useful starting point.\n",
    "2. Identify a vectorized-able code segment.\n",
    "3. Vectorize it.\n",
    "4. Run the cell again, note the runtime.\n",
    "5. Go to step 2 until satisfied or no more code can be vectorized.\n",
    "\n",
    "\n",
    "**Note.** Below, `topic_word_props_importance` simply means the usual topic-word proportions that are \"weighted\" (practically: divided) by their marginal over the topics. The idea is: Words that prominently presents in a topic and  rarely occurs in other topics are important to that topic. The intuition is similar to [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 20  # Number of samples to be obtained\n",
    "BURN_IN = 50  # Wait for k iterations before start taking samples\n",
    "THINNING = 5  # Take sample every k-th iteration\n",
    "\n",
    "N_ITER = BURN_IN + N_SAMPLES * THINNING\n",
    "\n",
    "\n",
    "# Sum topic-word-proportion and doc-topic-proportion over samples\n",
    "# Used to compute the expectation of them, for \"predictions\" and visualizations\n",
    "total_topic_word_props = 0\n",
    "total_topic_word_props_importance = 0\n",
    "total_doc_topic_props = 0\n",
    "n_samples = 0\n",
    "\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "# for it in tqdm.trange(N_ITER):\n",
    "for it in tq(N_ITER):\n",
    "    \n",
    "    # Sample from the full conditional of Theta\n",
    "    # -----------------------------------------\n",
    "    n_theta = np.sum(n, axis=0)\n",
    "    for k in range(K):\n",
    "        betas = np.ones(V) * beta\n",
    "        Theta[k, :] = np.random.dirichlet(betas + n_theta[k, :])\n",
    "    \n",
    "    # Sample from the full conditional of Pi\n",
    "    # --------------------------------------\n",
    "    n_pi = np.sum(n, axis=2)\n",
    "    for d in range(D):\n",
    "        alphas = np.ones(K) * alpha\n",
    "        Pi[d, :] = np.random.dirichlet(alphas + n_pi[d, :])\n",
    "        \n",
    "    # Sample from the full conditional of C\n",
    "    # -------------------------------------\n",
    "    \n",
    "    # Initialize C\n",
    "    C = list()\n",
    "    \n",
    "    # Initialize n\n",
    "    n = np.zeros((D, K, V))\n",
    "    \n",
    "    # Compute normalization constant\n",
    "    Z = Pi @ Theta\n",
    "    \n",
    "    for d in range(D):\n",
    "        \n",
    "        # Get next document (list of words)\n",
    "        w = W[d]\n",
    "        \n",
    "        # Get document length\n",
    "        Id = len(w)\n",
    "        \n",
    "        # Initialize c for document d\n",
    "        c = np.zeros((Id, K))\n",
    "        \n",
    "        # Get Dirichlet distribution for pi\n",
    "        pi = Pi[d, :]\n",
    "    \n",
    "        for i in range(Id):\n",
    "            \n",
    "            # Get word index in the vocabulary\n",
    "            v = w[i]\n",
    "            \n",
    "            # Get Dirichlet distribution for theta\n",
    "            theta = Theta[:, v]\n",
    "\n",
    "            # Get normalization contant\n",
    "            z = Z[d, v]\n",
    "            \n",
    "            # Get probability distribution\n",
    "            p = pi * theta.T / z\n",
    "            \n",
    "            # Get k from a categorical distribution\n",
    "            k = np.random.choice(ks, p=p)\n",
    "            \n",
    "            # Assign topic to word in a document\n",
    "            c[i, k] = 1\n",
    "        \n",
    "            # Update (document, topic, word) counter\n",
    "            n[d, k, v] += 1\n",
    "        \n",
    "        # Append list\n",
    "        C.append(c)\n",
    "            \n",
    "    # Take samples and compute the running sums\n",
    "    # -----------------------------------------\n",
    "    if it >= BURN_IN:\n",
    "        if (it - BURN_IN) % THINNING == 0:\n",
    "            total_topic_word_props += Theta\n",
    "            total_topic_word_props_importance += Theta/Theta.sum(0, keepdims=True)\n",
    "            total_doc_topic_props += Pi\n",
    "            n_samples += 1\n",
    "            \n",
    "            \n",
    "print(f'Obtained {n_samples} samples')\n",
    "            \n",
    "            \n",
    "# Obtain the expected topic-word-proportion and doc-topic-proportion over samples\n",
    "# -------------------------------------------------------------------------------\n",
    "topic_word_props = total_topic_word_props / n_samples\n",
    "topic_word_props_importance = total_topic_word_props_importance / n_samples\n",
    "doc_topic_props = total_doc_topic_props / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your implementation, whether it yields some nice/coherent topics or not. This is a quick indicator whether the inference is successful or not. More quantitative metrics exist, but they are outside of the scope of this exercise.\n",
    "\n",
    "**TASK 3.** For each topic, print out `n_top` of the most probable important words, as encoded in `topic_word_props_importance`. Example outputs (`n_top = 8`):\n",
    "\n",
    "```\n",
    "Topic-1\n",
    "enemy army war ship fight navy sea supply\n",
    "\n",
    "Topic-2\n",
    "currency gold treasury article convention payment minister delay\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 8\n",
    "\n",
    "##########################################################\n",
    "##                                                      ##\n",
    "##                  YOUR CODE HERE                      ##\n",
    "##                                                      ##\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the lecture, latent semantic analysis/indexing (LSA/LSI)---a topic-modelling algorithm based on matrix-decomposition---yields _non-sparse_ solutions.\n",
    "\n",
    "\n",
    "**TASK 4.** Show that LDA yields _sparse_ **document-topic** and **topic-word proportions**. Plot some of these proportions, so that we can visually inspect the sparsity of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##                                                      ##\n",
    "##                  YOUR CODE HERE                      ##\n",
    "##                                                      ##\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it would be interesting to see the change of topics over time.\n",
    "\n",
    "**TASK 5.** Plot the document-topic proportion sequentially, ordered by year. See Fig. 5 in https://arxiv.org/abs/1110.4713 for an example.\n",
    "\n",
    "**Hint.** Remember that we have the variable `years` (cf. cell 2 and 3) containing the corresponding year of the documents (the document list might not be ordered!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "##                                                      ##\n",
    "##                  YOUR CODE HERE                      ##\n",
    "##                                                      ##\n",
    "##########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
