{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3: Battleships (2/2)\n",
    "\n",
    "### Probabilistic Machine Learning\n",
    "\n",
    "- **Lecturer**: Prof. Philipp Hennig\n",
    "- **Term**: SoSe 2020\n",
    "- **Due Date**: Monday, 11 May 2020\n",
    "\n",
    "\n",
    "![battleship rules](https://upload.wikimedia.org/wikipedia/commons/e/e4/Battleships_Paper_Game.svg)\n",
    "\n",
    "In the previous exercise sheet on the game _battleships_, we studied prior and posterior probabilities of getting a hit by enumerating all possible locations of one boat. We established that it is computationally unfeasible to obtain the full posterior by enumeration for the number of ships in the rules.\n",
    "\n",
    "Hence, to render the problem tractable, we need to approximate the posterior over hits given hit/miss observations. We achieve this through _Monte Carlo_ sampling. Instead of enumerating _all_ board states, we randomly sample `nb_samples` valid board states (i.e. configurations of ship arrangements on the board) and use their sum to estimate the posterior.\n",
    "\n",
    "__The goal of this assignment is to write such a  Monte Carlo sampler.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3.1__ Getting started\n",
    "\n",
    "There are a few accompanying files that are required for the game. You will only need to alter `MC_agent.py`, all other vital routines are provided.\n",
    "- `main.py`: Run this script to play the game. You can select which players should play against each other: `\"human\"`, `\"random\"`, or `\"MC\"`. The first two are already implemented, the latter is our task here.\n",
    "- `board.py`: contains a class to track the current state of the board (observations), and to save the placement of the ships.  \n",
    "- `game.py`: the game itself with relevant methods (initialize the game, calling the respective agents for their moves, checking if the game is over, etc.)\n",
    "- `human_agent.py`: The human agent (interactive input)\n",
    "- `random_agent.py`: The random agent (just selects a previously unobserved location at random)\n",
    "- `MC_Agent`: this is where you should implement the Monte Carlo Agent for the virtual opponent. Initially, it randomly selects a query location (i.e. you can play the game by running `main.py`, but the computer makes uninformed moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current state, you can already play the game against a random agent, or have two random agents play against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from main import play_battleships\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Player1's observations   Player2's observations\n",
      "\n",
      "  0 1 2 3 4 5 6 7 8 9      0 1 2 3 4 5 6 7 8 9\n",
      "0 . . X . . . . X . .    0 . . . . X - . . . .\n",
      "1 . . X . . . . X . .    1 . . . . X X . . - .\n",
      "2 . X X X X . . X . .    2 . . . . . . . . . .\n",
      "3 . . - . . . . . X .    3 . . . - . . . . X .\n",
      "4 X . . . . . . . X .    4 - . . . . . . X - .\n",
      "5 . - X X X X X . X .    5 . . . . . . . X - .\n",
      "6 - . - . . . . . . .    6 . . . . X X . X X .\n",
      "7 . . . . . - . . - .    7 . - . . . . . X . .\n",
      "8 . . . . . . . - . .    8 . . . . X X X X . X\n",
      "9 . . . . . . . . . -    9 . . - . . . . . . .\n",
      "Game over! Player 1 won!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_battleships('random', 'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3.2__ Implement the Monte Carlo Agent\n",
    "\n",
    "Your task is to replace the method `select_observation` in the class `MCAgent` by a routine that samples board states to estimate the posterior probability of getting a hit, and then choosing the location that maximizes this probability.\n",
    "\n",
    "The state of observations is saved in the variable `self.observed_board`. A `0` is an unobserved location, `-1` a miss or a sunk ship, and a `1` a hit of a ship that is not yet sunk.\n",
    "\n",
    "In `select_observation`, we first define a variable `score_board` through which we will assign scores to each field. Therefore it has the shape of the `self.observed_board`.\n",
    "\n",
    "_The key idea of the Monte Carlo agent is to sum up many possible boat placements and choose the location with the largest score (most boats placed) as new query location._\n",
    "\n",
    "Here is how to proceed (see comments in the function):\n",
    "1. Check if there is an \"open\" hit, i.e. a hit that does not belong to a sunk ship. If so, reduce the number of samples (e.g. to a tenth) and force possible boats to pass through the hit(s)\n",
    "2. In case there is no open hit, simulate `nb_samples` boats, the location and orientation of which are chosen at random. Track them using the `score_board`.\n",
    "3. Find the location with the largest score and return its indices.\n",
    "\n",
    "Once you're done, you can play against your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Player1's observations   Player2's observations\n",
      "\n",
      "  0 1 2 3 4 5 6 7 8 9      0 1 2 3 4 5 6 7 8 9\n",
      "0 . X X X . X . . - .    0 . . . . . . . . . .\n",
      "1 . . . . . X . . . .    1 . . . . . . . . . .\n",
      "2 . . . . . . . . - .    2 . . . . . . . . . .\n",
      "3 . . . . . . . . . .    3 X . . . . . . . . .\n",
      "4 . - . . . . . . . .    4 X . . . . . X . . .\n",
      "5 X X X . . . . . . .    5 X X X . . . . X . .\n",
      "6 . . . . . . . . X .    6 X X X . . . . X . .\n",
      "7 . . - - X X . . . .    7 X X X . . . . . . .\n",
      "8 . . . - . . . . . .    8 . . X . . . . X . .\n",
      "9 . . . X X X X X . .    9 . . X X - - - - - -\n",
      "Game over! Player 2 won!\n"
     ]
    }
   ],
   "source": [
    "# play_battleships('MC', 'human')\n",
    "\n",
    "n_games = 30\n",
    "\n",
    "counter = {\n",
    "    True: 0,\n",
    "    False: 0,\n",
    "    'draw': 0\n",
    "}\n",
    "\n",
    "for _ in range(n_games):\n",
    "    winner = play_battleships('random', 'MC')\n",
    "    if winner is None:\n",
    "        counter['draw'] += 1\n",
    "    else:\n",
    "        counter[winner] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random player won 6 / 30 (20.00%)\n",
      "MC player won 23 / 30 (76.67%)\n",
      "Number of draws 1 / 30 (3.33%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Random player won {counter[1]} / {n_games} ({counter[1] / n_games * 100:.2f}%)\")\n",
    "print(f\"MC player won {counter[0]} / {n_games} ({counter[0] / n_games * 100:.2f}%)\")\n",
    "print(f\"Number of draws {counter['draw']} / {n_games} ({counter['draw'] / n_games * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3.2__ Further analysis\n",
    "__Task 1:__  \n",
    "Surface the score matrix of your Monte Carlo agent and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2:__  \n",
    "How could you evaluate your agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We can count the number/proportion of times it won against a random and human player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3:__  \n",
    "How could you improve your agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: It depends how deep do you want to go in solving this problem...\n",
    "- Changing the current MC algorithm implementation with a better MC method might potentially improve the performance of the agent.\n",
    "- Another change would be using reinforcement-learning methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
